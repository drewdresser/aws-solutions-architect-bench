{"input": "A company runs a public-facing three-tier web application in a VPC across multiple Availability Zones. Amazon EC2 instances for the application tier running in private subnets need to download software patches from the internet. However, the EC2 instances cannot be directly accessible from the internet. Which actions should be taken to allow the EC2 instances to download the needed patches? (Select TWO.)", "choices": ["Configure a NAT gateway in a public subnet.", "Define a custom route table with a route to the NAT gateway for internet traffic and associate it with the private subnets for the application tier.", "Assign Elastic IP addresses to the EC2 instances.", "Define a custom route table with a route to the internet gateway for internet traffic and associate it with the private subnets for the application tier.", "Configure a NAT instance in a private subnet."], "target": ["A", "B"], "metadata": {"difficulty": "sa_associate", "domain": "networking", "aws_services": ["AWS SSO", "Amazon EC2", "Amazon VPC", "Elastic IP", "NAT Gateway"]}}
{"input": "A solutions architect wants to design a solution to save costs for Amazon EC2 instances that do not need to run during a 2-week company shutdown. The applications running on the EC2 instances store data in instance memory that must be present when the instances resume operation. Which approach should the solutions architect recommend to shut down and resume the EC2 instances?", "choices": ["Modify the application to store the data on instance store volumes. Reattach the volumes while restarting them.", "Snapshot the EC2 instances before stopping them. Restore the snapshot after restarting the instances.", "Run the applications on EC2 instances enabled for hibernation. Hibernate the instances before the 2-week company shutdown.", "Note the Availability Zone for each EC2 instance before stopping it. Restart the instances in the same Availability Zones after the 2-week company shutdown."], "target": "C", "metadata": {"difficulty": "sa_associate", "domain": "compute", "aws_services": ["Amazon EC2"]}}
{"input": "A company plans to run a monitoring application on an Amazon EC2 instance in a VPC. Connections are made to the EC2 instance using the instance\u2019s private IPv4 address. A solutions architect needs to design a solution that will allow traffic to be quickly directed to a standby EC2 instance if the application fails and becomes unreachable. Which approach will meet these requirements?", "choices": ["Deploy an Application Load Balancer configured with a listener for the private IP address and register the primary EC2 instance with the load balancer. Upon failure, de-register the instance and register the standby EC2 instance.", "Configure a custom DHCP option set. Configure DHCP to assign the same private IP address to the standby EC2 instance when the primary EC2 instance fails.", "Attach a secondary elastic network interface to the EC2 instance configured with the private IP address. Move the network interface to the standby EC2 instance if the primary EC2 instance becomes unreachable.", "Associate an Elastic IP address with the network interface of the primary EC2 instance. Disassociate the Elastic IP from the primary instance upon failure and associate it with a standby EC2 instance."], "target": "C", "metadata": {"difficulty": "sa_associate", "domain": "networking", "aws_services": ["AWS SSO", "Amazon EC2", "Amazon VPC", "Elastic IP", "Elastic Load Balancer"]}}
{"input": "An analytics company is planning to offer a web-analytics service to its users. The service will require that the users\u2019 webpages include a JavaScript script that makes authenticated GET requests to the company\u2019s Amazon S3 bucket. What must a solutions architect do to ensure that the script will successfully execute?", "choices": ["Enable cross-origin resource sharing (CORS) on the S3 bucket.", "Enable S3 Versioning on the S3 bucket.", "Provide the users with a signed URL for the script.", "Configure an S3 bucket policy to allow public execute privileges."], "target": "A", "metadata": {"difficulty": "sa_associate", "domain": "storage", "aws_services": ["Amazon S3"]}}
{"input": "A company\u2019s security team requires that all data stored in the cloud be encrypted at rest at all times using encryption keys stored on premises. Which encryption options meet these requirements? (Select TWO.)", "choices": ["Use server-side encryption with Amazon S3 managed encryption keys (SSE-S3).", "Use server-side encryption with AWS KMS managed encryption keys (SSE-KMS).", "Use server-side encryption with customer-provided encryption keys (SSE-C).", "Use client-side encryption to provide at-rest encryption.", "Use an AWS Lambda function invoked by Amazon S3 events to encrypt the data using the customer\u2019s keys."], "target": ["C", "D"], "metadata": {"difficulty": "sa_associate", "domain": "security", "aws_services": ["AWS KMS", "AWS Lambda", "Amazon S3"]}}
{"input": "A company uses Amazon EC2 Reserved Instances to run its data-processing workload. The nightly job typically takes 7 hours to run and must finish within a 10-hour time window. The company anticipates temporary increases in demand at the end of each month that will cause the job to run over the time limit with the capacity of the current resources. Once started, the processing job cannot be interrupted before completion. The company wants to implement a solution that would provide increased resource capacity as cost-effectively as possible. What should a solutions architect do to accomplish this?", "choices": ["Deploy On-Demand Instances during periods of high demand.", "Create a second EC2 reservation for additional instances.", "Deploy Spot Instances during periods of high demand.", "Increase the EC2 instance size in the EC2 reservation to support the increased workload."], "target": "A", "metadata": {"difficulty": "sa_associate", "domain": "cost", "aws_services": ["Amazon EC2"]}}
{"input": "A company runs an online voting system for a weekly live television program. During broadcasts, users submit hundreds of thousands of votes within minutes to a front-end fleet of Amazon EC2 instances that run in an Auto Scaling group. The EC2 instances write the votes to an Amazon RDS database. However, the database is unable to keep up with the requests that come from the EC2 instances. A solutions architect must design a solution that processes the votes in the most efficient manner and without downtime. Which solution meets these requirements?", "choices": ["Migrate the front-end application to AWS Lambda. Use Amazon API Gateway to route user requests to the Lambda functions.", "Scale the database horizontally by converting it to a Multi-AZ deployment. Configure the front-end application to write to both the primary and secondary DB instances.", "Configure the front-end application to send votes to an Amazon Simple Queue Service (Amazon SQS) queue. Provision worker instances to read the SQS queue and write the vote information to the database.", "Use Amazon EventBridge to create a scheduled event to re-provision the database with larger, memory-optimized instances during voting periods. When voting ends, re-provision the database to use smaller instances."], "target": "C", "metadata": {"difficulty": "sa_associate", "domain": "messaging", "aws_services": ["AWS Lambda", "Amazon API Gateway", "Amazon EC2", "Amazon RDS", "Amazon SQS", "Auto Scaling"]}}
{"input": "A company has a two-tier application architecture that runs in public and private subnets. Amazon EC2 instances running the web application are in the public subnet and an EC2 instance for the database runs on the private subnet. The web application instances and the database are running in a single Availability Zone (AZ). Which combination of steps should a solutions architect take to provide high availability for this architecture? (Select TWO.)", "choices": ["Create new public and private subnets in the same AZ.", "Create an Amazon EC2 Auto Scaling group and Application Load Balancer spanning multiple AZs for the web application instances.", "Add the existing web application instances to an Auto Scaling group behind an Application Load Balancer.", "Create new public and private subnets in a new AZ. Create a database using an EC2 instance in the public subnet in the new AZ. Migrate the old database contents to the new database.", "Create new public and private subnets in the same VPC, each in a new AZ. Create an Amazon RDS Multi-AZ DB instance in the private subnets. Migrate the old database contents to the new DB instance."], "target": ["B", "E"], "metadata": {"difficulty": "sa_associate", "domain": "high_availability", "aws_services": ["Amazon EC2", "Amazon RDS", "Amazon VPC", "Auto Scaling", "Elastic Load Balancer"]}}
{"input": "A website runs a custom web application that receives a burst of traffic each day at noon. The users upload new pictures and content daily, but have been complaining of timeouts. The architecture uses Amazon EC2 Auto Scaling groups, and the application consistently takes 1 minute to initiate upon boot up before responding to user requests. How should a solutions architect redesign the architecture to better respond to changing traffic?", "choices": ["Configure a Network Load Balancer with a slow-start configuration.", "Configure Amazon ElastiCache for Redis to offload direct requests from the EC2 instances.", "Configure an Auto Scaling step scaling policy with an EC2 instance warm-up condition.", "Configure Amazon CloudFront to use an Application Load Balancer as the origin."], "target": "C", "metadata": {"difficulty": "sa_associate", "domain": "scaling", "aws_services": ["Amazon EC2", "Amazon ElastiCache", "Auto Scaling", "Elastic Load Balancer"]}}
{"input": "An application running on AWS uses an Amazon Aurora Multi-AZ DB cluster deployment for its database. When evaluating performance metrics, a solutions architect discovered that the database reads are causing high I/O and adding latency to the write requests against the database. What should the solutions architect do to separate the read requests from the write requests?", "choices": ["Enable read-through caching on the Aurora database.", "Update the application to read from the Multi-AZ standby instance.", "Create an Aurora replica and modify the application to use the appropriate endpoints.", "Create a second Aurora database and link it to the primary database as a read replica."], "target": "C", "metadata": {"difficulty": "sa_associate", "domain": "database", "aws_services": ["Amazon Aurora"]}}
{"input": "A company has many AWS accounts that individual business groups own. One of the accounts was recently compromised. The attacker launched a large number of instances, resulting in a high bill for that account. The company addressed the security breach, but a solutions architect needs to develop a solution to prevent excessive spending in all accounts. Each business group wants to retain full control of its AWS account. Which solution should the solutions architect recommend to meet these requirements?", "choices": ["Use AWS Organizations. Add each AWS account to the management account. Create an SCP that uses the ec2:instanceType condition key to prevent the launch of high-cost instance types in each account.", "Attach a new customer-managed IAM policy to an IAM group in each account. Configure the policy to use the ec2:instanceType condition key to prevent the launch of high-cost instance types. Place all the existing IAM users in each group.", "Turn on billing alerts for each AWS account. Create Amazon CloudWatch alarms that send an Amazon SNS notification to the account administrator whenever the account exceeds a designated spending threshold.", "Turn on AWS Cost Explorer in each account. Review the Cost Explorer reports for each account on a regular basis to ensure that spending does not exceed the desired amount."], "target": "C", "metadata": {"difficulty": "sa_pro", "domain": "cost_management", "aws_services": ["AWS Organizations", "Amazon CloudWatch", "Amazon EC2", "Amazon SNS", "IAM"]}}
{"input": "A company has multiple AWS accounts in an organization in AWS Organizations. The company has integrated its on-premises Active Directory with AWS Single Sign-On (AWS SSO) to grant Active Directory users least-privilege permissions to manage infrastructure across all the accounts. A solutions architect must integrate a third-party monitoring solution that requires read-only access across all AWS accounts. The monitoring solution will run in its own AWS account. What should the solutions architect do to provide the monitoring solution with the required permissions?", "choices": ["Create a user in an AWS SSO directory. Assign a read-only permissions set to the user. Assign all AWS accounts that need monitoring to the user. Provide the third-party monitoring solution with the user name and password.", "Create an IAM role in the organization\u2019s management account. Allow the AWS account of the third-party monitoring solution to assume the role.", "Invite the AWS account of the third-party monitoring solution to join the organization. Enable all features.", "Create an AWS CloudFormation template that defines a new IAM role for the third-party monitoring solution. Specify the AWS account of the third-party monitoring solution in the trust policy. Create the IAM role across all linked AWS accounts by using a stack set."], "target": "D", "metadata": {"difficulty": "sa_pro", "domain": "security", "aws_services": ["AWS CloudFormation", "AWS Organizations", "AWS SSO", "IAM"]}}
{"input": "A team is building an HTML form that is hosted in a public Amazon S3 bucket. The form uses JavaScript to post data to an Amazon API Gateway API endpoint. The API endpoint is integrated with AWS Lambda functions. The team has tested each method in the API Gateway console and has received valid responses. Which combination of steps must the team complete so that the form can successfully post to the API endpoint and receive a valid response? (Select TWO.)", "choices": ["Configure the S3 bucket to allow cross-origin resource sharing (CORS).", "Host the form on Amazon EC2 rather than on Amazon S3.", "Request a quota increase for API Gateway.", "Enable cross-origin resource sharing (CORS) in API Gateway.", "Configure the S3 bucket for web hosting."], "target": ["D", "E"], "metadata": {"difficulty": "sa_pro", "domain": "networking", "aws_services": ["AWS Lambda", "Amazon API Gateway", "Amazon EC2", "Amazon S3"]}}
{"input": "A company runs a serverless mobile app that uses Amazon API Gateway, AWS Lambda functions, Amazon Cognito, and Amazon DynamoDB. During large surges in traffic, users report intermittent system failures. The API Gateway API endpoint is returning HTTP status code 502 (Bad Gateway) errors to valid requests. Which solution will resolve this issue?", "choices": ["Increase the concurrency quota for the Lambda functions. Configure Amazon CloudWatch to send notification alerts when the ConcurrentExecutions metric approaches the quota.", "Configure notification alerts for the quota of transactions per second on the API Gateway API endpoint. Create a Lambda function that will increase the quota when the quota is reached.", "Shard users to Amazon Cognito user pools in multiple AWS Regions to reduce user authentication latency.", "Use DynamoDB strongly consistent reads to ensure that the client application always receives the most recent data."], "target": "A", "metadata": {"difficulty": "sa_pro", "domain": "serverless", "aws_services": ["AWS Lambda", "Amazon API Gateway", "Amazon CloudWatch", "Amazon Cognito", "Amazon DynamoDB"]}}
{"input": "A company is launching a new web service on an Amazon Elastic Container Service (Amazon ECS) cluster. The cluster consists of 100 Amazon EC2 instances. Company policy requires the security group on the cluster instances to block all inbound traffic except HTTPS (port 443). Which solution will meet these requirements?", "choices": ["Change the SSH port to 2222 on the cluster instances by using a user data script. Log in to each instance by using SSH over port 2222.", "Change the SSH port to 2222 on the cluster instances by using a user data script. Use AWS Trusted Advisor to remotely manage the cluster instances over port 2222.", "Launch the cluster instances with no SSH key pairs. Use AWS Systems Manager Run Command to remotely manage the cluster instances.", "Launch the cluster instances with no SSH key pairs. Use AWS Trusted Advisor to remotely manage the cluster instances."], "target": "C", "metadata": {"difficulty": "sa_pro", "domain": "operations", "aws_services": ["AWS Systems Manager", "AWS Trusted Advisor", "Amazon EC2", "Amazon ECS", "Security Groups"]}}
{"input": "A company has two AWS accounts: one account for production workloads and one account for development workloads. A development team and an operations team create and manage these workloads. The company needs a security strategy that meets the following requirements: \u2022 Developers need to create and delete development application infrastructure. \u2022 Operators need to create and delete development and production application infrastructure. \u2022 Developers must have no access to production infrastructure. \u2022 All users must have a single set of AWS credentials. Which strategy will meet these requirements?", "choices": ["In the production account: \u2022 Create an operations IAM group that can create and delete application infrastructure. \u2022 Create an IAM user for each operator. Assign these users to the operations group. In the development account: \u2022 Create a development IAM group that can create and delete application infrastructure. \u2022 Create an IAM user for each operator and developer. Assign these users to the development group.", "In the production account: \u2022 Create an operations IAM group that can create and delete application infrastructure. In the development account: \u2022 Create a development IAM group that can create and delete application infrastructure. \u2022 Create an IAM user for each developer. Assign these users to the development group. \u2022 Create an IAM user for each operator. Assign these users to the development group and to the operations group in the production account.", "In the development account: \u2022 Create a shared IAM role that can create and delete application infrastructure in the production account. \u2022 Create a development IAM group that can create and delete application infrastructure. \u2022 Create an operations IAM group that can assume the shared role. \u2022 Create an IAM user for each developer. Assign these users to the development group. \u2022 Create an IAM user for each operator. Assign these users to the development group and to the operations group.", "In the production account: \u2022 Create a shared IAM role that can create and delete application infrastructure. \u2022 Add the development account to the trust policy for the shared role. In the development account: \u2022 Create a development IAM group that can create and delete application infrastructure. \u2022 Create an operations IAM group that can assume the shared role in the production account. \u2022 Create an IAM user for each developer. Assign these users to the development group. \u2022 Create an IAM user for each operator. Assign these users to the development group and to the operations group."], "target": "D", "metadata": {"difficulty": "sa_pro", "domain": "iam", "aws_services": ["IAM"]}}
{"input": "A solutions architect needs to reduce costs for a big data application. The application environment consists of hundreds of devices that send events to Amazon Kinesis Data Streams. The device ID is used as the partition key, so each device gets a separate shard. Each device sends between 50 KB and 450 KB of data each second. An AWS Lambda function polls the shards, processes the data, and stores the result in Amazon S3. Every hour, another Lambda function runs an Amazon Athena query against the result data to identify outliers. This Lambda function places the outliers in an Amazon Simple Queue Service (Amazon SQS) queue. An Amazon EC2 Auto Scaling group of two EC2 instances monitors the queue and runs a 30-second process to address the outliers. The devices submit an average of 10 outlying values every hour. Which combination of changes to the application will MOST reduce costs? (Select TWO.)", "choices": ["Change the Auto Scaling group launch configuration to use smaller instance types in the same instance family.", "Replace the Auto Scaling group with a Lambda function that is invoked when messages arrive in the queue.", "Reconfigure the devices and data stream to set a ratio of 10 devices to 1 data stream shard.", "Reconfigure the devices and data stream to set a ratio of 2 devices to 1 data stream shard.", "Change the desired capacity of the Auto Scaling group to a single EC2 instance."], "target": ["B", "D"], "metadata": {"difficulty": "sa_pro", "domain": "big_data", "aws_services": ["AWS Lambda", "Amazon Athena", "Amazon EC2", "Amazon Kinesis", "Amazon RDS", "Amazon S3", "Amazon SQS", "Auto Scaling"]}}
{"input": "A company operates an ecommerce application on Amazon EC2 instances behind an Application Load Balancer. The instances run in an Amazon EC2 Auto Scaling group across multiple Availability Zones. After an order is successfully processed, the application immediately posts order data to a third-party affiliate\u2019s external tracking system that pays sales commissions for order referrals. During a successful marketing promotion, the number of EC2 instances increased from 2 to 20. The application continued to work correctly during this time. However, the increased request rate overwhelmed the third-party affiliate and resulted in failed requests. Which combination of architectural changes should a solutions architect make to ensure that the entire process functions correctly under load? (Select TWO.)", "choices": ["Move the code that calls the affiliate to a new AWS Lambda function. Modify the application to invoke the Lambda function asynchronously.", "Move the code that calls the affiliate to a new AWS Lambda function. Modify the application to place the order data in an Amazon Simple Queue Service (Amazon SQS) queue. Invoke the Lambda function from the queue.", "Increase the timeout of the new AWS Lambda function.", "Decrease the reserved concurrency of the new AWS Lambda function.", "Increase the memory of the new AWS Lambda function."], "target": ["B", "D"], "metadata": {"difficulty": "sa_pro", "domain": "architecture", "aws_services": ["AWS Lambda", "Amazon EC2", "Amazon ECR", "Amazon SQS", "Auto Scaling", "Elastic Load Balancer"]}}
{"input": "A company has built an online ticketing web application on AWS. The application is hosted on AWS App Runner and uses images that are stored in an Amazon Elastic Container Registry (Amazon ECR) repository. The application stores data in an Amazon Aurora MySQL DB cluster. The company has set up a domain name in Amazon Route 53. The company needs to deploy the application across two AWS Regions in an active-active configuration. Which combination of steps will meet these requirements with the LEAST change to the architecture? (Select THREE.)", "choices": ["Set up Cross-Region Replication to the second Region for the ECR images.", "Create a VPC endpoint from the ECR repository in the second Region.", "Edit the App Runner configuration by adding a second deployment target to the second Region.", "Deploy App Runner to the second Region. Set up Route 53 latency-based routing.", "Change the database by using Amazon DynamoDB global tables in the two desired Regions.", "Use an Aurora global database with write forwarding enabled in the second Region."], "target": ["A", "D", "F"], "metadata": {"difficulty": "sa_pro", "domain": "global_architecture", "aws_services": ["AWS App Runner", "Amazon Aurora", "Amazon DynamoDB", "Amazon ECR", "Amazon Route 53", "Amazon VPC"]}}
{"input": "A company has deployed a multi-tier web application in the AWS Cloud. The application consists of the following tiers: \u2022 A Windows-based web tier that is hosted on Amazon EC2 instances with Elastic IP addresses \u2022 A Linux-based application tier that is hosted on EC2 instances that run behind an Application Load Balancer (ALB) that uses path-based routing \u2022 A MySQL database that runs on a Linux EC2 instance. All the EC2 instances are using Intel-based x86 CPUs. A solutions architect needs to modernize the infrastructure to achieve better performance. The solution must minimize the operational overhead of the application. Which combination of actions should the solutions architect take to meet these requirements? (Select TWO.)", "choices": ["Run the MySQL database on multiple EC2 instances.", "Place the web tier instances behind an ALB.", "Migrate the MySQL database to Amazon Aurora Serverless.", "Migrate all EC2 instance types to Graviton2.", "Replace the ALB for the application tier instances with a company-managed load balancer."], "target": ["B", "C"], "metadata": {"difficulty": "sa_pro", "domain": "modernization", "aws_services": ["Amazon Aurora", "Amazon EC2", "Elastic IP", "Elastic Load Balancer"]}}
{"input": "Your company needs to ensure that all Amazon S3 buckets are configured to block public access in order to comply with internal security policies. Which AWS service can be used to automatically evaluate the configuration of your S3 buckets and notify you if they do not comply with this requirement? (Select ONE.)", "choices": ["Amazon CloudWatch", "AWS Config", "AWS Trusted Advisor", "AWS CloudTrail"], "target": "B", "metadata": {"difficulty": "sa_associate", "domain": "compliance", "aws_services": ["Amazon S3", "AWS Config", "Amazon CloudWatch", "AWS Trusted Advisor", "AWS CloudTrail"]}}
{"input": "Your organization wants to enhance its cloud security posture by implementing AWS services that can help with threat detection and compliance. Which services should you consider using to achieve this goal? (Select TWO.)", "choices": ["AWS Lambda", "Amazon GuardDuty", "AWS Security Hub", "Amazon RDS", "AWS CloudFormation"], "target": ["B", "C"], "metadata": {"difficulty": "sa_associate", "domain": "compliance", "aws_services": ["Amazon GuardDuty", "AWS Security Hub"]}}
{"input": "Your company has a large data lake stored in Amazon S3. You want to use Amazon Athena to query the data without moving it to another location. Which of the following steps is necessary to allow Athena to access and query the data in your S3 bucket?", "choices": ["Create a VPC endpoint for Amazon S3.", "Grant Amazon Athena permission to access the data in Amazon S3 through IAM roles or policies.", "Enable S3 Transfer Acceleration on the bucket.", "Use Amazon EMR to preprocess the data before querying with Athena."], "target": "B", "metadata": {"difficulty": "sa_associate", "domain": "data_analytics", "aws_services": ["Amazon Athena", "Amazon S3", "IAM"]}}
{"input": "An organization needs to perform ETL operations on large datasets stored in Amazon S3. They also want to maintain a centralized metadata repository to track the schema and data locations. Which AWS service should they use to achieve this? (Select ONE.)", "choices": ["Amazon Athena", "AWS Glue", "Amazon Redshift", "Amazon EMR"], "target": "B", "metadata": {"difficulty": "sa_associate", "domain": "data_analytics", "aws_services": ["Amazon S3", "AWS Glue", "Amazon Athena", "Amazon Redshift", "Amazon EMR"]}}
{"input": "Your company wants to implement a disaster recovery strategy for its application running on Amazon EC2 instances. The requirement is to back up the data to a different AWS region to ensure data durability and availability in case of a regional failure. Which AWS service should you use to automate this backup process?", "choices": ["Amazon EC2 Auto Scaling", "AWS Backup", "Amazon CloudWatch", "AWS Elastic Beanstalk"], "target": "B", "metadata": {"difficulty": "sa_associate", "domain": "disaster_recovery", "aws_services": ["Amazon EC2", "AWS Backup"]}}
{"input": "A company is designing a disaster recovery plan for their critical applications running on Amazon EC2 instances. They need a solution that minimizes recovery time objective (RTO) and recovery point objective (RPO). Which AWS services should they use to achieve this goal? (Select TWO.)", "choices": ["Amazon RDS with Multi-AZ deployments", "Amazon EC2 Auto Scaling", "AWS Elastic Disaster Recovery", "Amazon S3 Glacier", "AWS Backup"], "target": ["C", "E"], "metadata": {"difficulty": "sa_associate", "domain": "disaster_recovery", "aws_services": ["Amazon EC2", "AWS Elastic Disaster Recovery", "AWS Backup", "Amazon RDS", "Amazon S3 Glacier"]}}
{"input": "A company is planning to migrate its on-premises MySQL database to AWS. They want to use AWS Database Migration Service (DMS) for this purpose. Which of the following is a required component to successfully set up DMS for this migration? (Select ONE.)", "choices": ["Amazon RDS as a source endpoint", "A replication instance", "Amazon S3 as a target endpoint", "AWS Lambda to transform data during migration"], "target": "B", "metadata": {"difficulty": "sa_associate", "domain": "migration", "aws_services": ["AWS Database Migration Service", "Amazon RDS", "Amazon S3", "AWS Lambda"]}}
{"input": "A company needs to transfer 80TB of data from their on-premises data center to AWS. They have a slow internet connection, and the transfer needs to be completed within a week. Which AWS service should they use to achieve this? (Select ONE.)", "choices": ["AWS DataSync", "AWS Snowball", "Amazon S3 Transfer Acceleration", "AWS Direct Connect"], "target": "B", "metadata": {"difficulty": "sa_associate", "domain": "migration", "aws_services": ["AWS Snowball", "AWS DataSync", "Amazon S3 Transfer Acceleration", "AWS Direct Connect"]}}
{"input": "Your company runs a web application on an Amazon EC2 instance, and you want to monitor the CPU utilization to ensure it stays below 80%. Which AWS service can you use to automatically notify you when the CPU utilization exceeds this threshold?", "choices": ["AWS Lambda", "Amazon CloudWatch", "AWS CloudTrail", "Amazon S3"], "target": "B", "metadata": {"difficulty": "sa_associate", "domain": "monitoring", "aws_services": ["Amazon EC2", "Amazon CloudWatch"]}}
{"input": "You are developing a microservices application running on Amazon ECS and you need to implement distributed tracing to gain insights into the performance of your application. Which AWS services should you use to achieve this? (Select TWO.)", "choices": ["AWS CloudTrail", "Amazon CloudWatch Logs", "AWS X-Ray", "Amazon SNS", "AWS Lambda"], "target": ["B", "C"], "metadata": {"difficulty": "sa_associate", "domain": "monitoring", "aws_services": ["Amazon ECS", "AWS X-Ray", "Amazon CloudWatch Logs"]}}
{"input": "An enterprise is looking to optimize its AWS spending. They want to analyze their historical AWS costs and usage to identify opportunities for savings, and set alerts for when costs exceed predefined budgets. Which AWS service should they primarily use to accomplish this? (Select ONE.)", "choices": ["AWS CloudTrail", "AWS Cost Explorer", "AWS CloudWatch", "AWS Trusted Advisor"], "target": "B", "metadata": {"difficulty": "sa_associate", "domain": "management", "aws_services": ["AWS Cost Explorer", "AWS CloudTrail", "AWS CloudWatch", "AWS Trusted Advisor"]}}
{"input": "Your company has multiple VPCs in different AWS regions that need to communicate with each other. You want to simplify the network architecture by reducing the number of VPC peering connections required. Which service would you use to efficiently connect all VPCs across regions? (Choose ONE.)", "choices": ["AWS Direct Connect", "VPC Peering", "AWS Transit Gateway", "AWS VPN"], "target": "C", "metadata": {"difficulty": "sa_associate", "domain": "networking", "aws_services": ["AWS Transit Gateway", "VPC Peering", "AWS Direct Connect", "AWS VPN"]}}
{"input": "You are designing a network architecture for your company that involves multiple VPCs in the same AWS Region. You need to ensure that these VPCs can communicate with each other over private IP addresses. Which of the following are necessary steps to achieve this? (Select TWO.)", "choices": ["Create a VPC peering connection between the VPCs.", "Enable VPC flow logs to track traffic between the VPCs.", "Update the route tables in each VPC to include routes for the peered VPC CIDR blocks.", "Enable internet gateways on each VPC to allow communication.", "Attach an AWS Transit Gateway to each VPC."], "target": ["A", "C"], "metadata": {"difficulty": "sa_associate", "domain": "networking", "aws_services": ["Amazon VPC"]}}
{"input": "Your company needs a shared file storage solution that can be accessed by multiple Amazon EC2 instances across multiple Availability Zones within the same AWS Region. Which AWS service should you choose for this requirement?", "choices": ["Amazon S3", "Amazon EFS", "Amazon Elastic Block Store (EBS)", "AWS Storage Gateway"], "target": "B", "metadata": {"difficulty": "sa_associate", "domain": "storage", "aws_services": ["Amazon EC2", "Amazon EFS", "Amazon S3", "Amazon Elastic Block Store", "AWS Storage Gateway"]}}
{"input": "Your company is looking to integrate their on-premises data with AWS using a hybrid cloud storage solution. They need to access their stored files seamlessly from both on-premises and the cloud while minimizing latency. Which AWS service should they use to achieve this? (Select ONE.)", "choices": ["Amazon S3 Glacier", "AWS Storage Gateway", "Amazon Elastic File System (Amazon EFS)", "Amazon RDS"], "target": "B", "metadata": {"difficulty": "sa_associate", "domain": "storage", "aws_services": ["AWS Storage Gateway", "Amazon S3 Glacier", "Amazon Elastic File System (Amazon EFS)", "Amazon RDS"]}}
{"input": "Your company operates an enterprise application that must maintain high availability across multiple regions with an RPO of less than 1 minute. The application consists of a web tier running on Amazon EC2 instances, a database tier using Amazon RDS for MySQL, and static content hosted in Amazon S3. You need to design an active-active disaster recovery architecture to meet these requirements while optimizing for cost. Which of the following solutions best meets these requirements? (Select ONE.)", "choices": ["Deploy the application in two regions with Amazon RDS Multi-AZ for MySQL in each region, use Amazon S3 Cross-Region Replication for static content, and configure Route 53 with latency-based routing to distribute traffic.", "Deploy the application in two regions with Amazon Aurora Global Database for the database tier, use Amazon S3 Cross-Region Replication for static content, and configure Route 53 with latency-based routing to distribute traffic.", "Deploy the application in two regions with a read replica of Amazon RDS for MySQL in the secondary region, use Amazon S3 Transfer Acceleration for static content, and configure Route 53 with geolocation routing to distribute traffic.", "Deploy the application in a single region with Amazon RDS Multi-AZ for MySQL, use Amazon S3 Cross-Region Replication for static content, and configure Route 53 with failover routing to direct traffic to a cold standby in another region."], "target": "B", "metadata": {"difficulty": "sa_pro", "domain": "disaster_recovery", "aws_services": ["Amazon EC2", "Amazon RDS", "Amazon S3", "Amazon Aurora", "Amazon Route 53"]}}
{"input": "Your organization needs to implement a disaster recovery strategy for a critical business application hosted on AWS. The application is composed of several Amazon EC2 instances, an Amazon RDS database, and static content stored in Amazon S3. Management has mandated a Recovery Time Objective (RTO) of 2 hours and a Recovery Point Objective (RPO) of 1 hour, with cost optimization as a secondary concern. Which of the following strategies would best meet these requirements? (Select TWO.)", "choices": ["Use a cold standby strategy with regular backups to Amazon S3 and launch resources on-demand during a disaster.", "Implement a pilot light strategy, keeping a minimal version of the environment always running and scale up resources during a disaster.", "Set up a warm standby environment with reduced-capacity instances running at all times across multiple Availability Zones.", "Use an active-active strategy, maintaining full production capacity in a separate AWS region to ensure zero downtime.", "Utilize AWS Backup to automate backup of EC2, RDS, and S3, and restore manually during a disaster."], "target": ["B", "C"], "metadata": {"difficulty": "sa_pro", "domain": "disaster_recovery", "aws_services": ["Amazon EC2", "Amazon RDS", "Amazon S3", "AWS Backup"]}}
{"input": "An organization is using AWS Organizations to manage multiple AWS accounts. They need to ensure compliance with data residency requirements that mandate certain data must not be stored outside of specific AWS Regions. The organization should also prevent accidental creation of resources in non-compliant regions across all accounts. Which strategy best achieves these requirements? (Select ONE.)", "choices": ["Create Service Control Policies (SCPs) that explicitly deny actions to create and manage resources in non-compliant regions for all accounts.", "Use AWS Config rules to detect resources created in non-compliant regions and automatically delete them.", "Set up AWS CloudTrail with trails configured to log only activities in compliant regions.", "Implement AWS IAM policies in each account to restrict resource creation to compliant regions."], "target": "A", "metadata": {"difficulty": "sa_pro", "domain": "compliance", "aws_services": ["AWS Organizations", "AWS Config", "AWS CloudTrail", "IAM"]}}
{"input": "An enterprise is running multiple AWS accounts for different business units and wants to ensure compliance by auditing all API activity across these accounts. They need to centralize logging and ensure logs are immutable and accessible for audit purposes. Which solutions can help achieve these requirements? (Select TWO.)", "choices": ["Enable AWS CloudTrail in each account and configure it to deliver logs to an Amazon S3 bucket in a centralized logging account with Object Lock enabled.", "Aggregate logs from AWS CloudTrail using AWS Lambda in each account and forward them to Amazon Kinesis Data Firehose for delivery to Amazon S3.", "Use AWS Organizations to enable CloudTrail logging across all accounts and store the logs in an Amazon S3 bucket with a retention policy.", "Deploy AWS Config in each account and configure it to deliver configuration snapshots to a centralized Amazon S3 bucket.", "Utilize Amazon CloudWatch Logs Insights to query CloudTrail logs in each account for compliance reporting."], "target": ["A", "C"], "metadata": {"difficulty": "sa_pro", "domain": "compliance", "aws_services": ["AWS CloudTrail", "Amazon S3", "AWS Organizations", "AWS Lambda", "Amazon Kinesis Data Firehose", "AWS Config", "Amazon CloudWatch Logs Insights"]}}
{"input": "Your company is building a multi-tenant application that offers APIs to its clients. You need to ensure secure access to the APIs and manage microservices communication with minimal latency. The architecture requires using Amazon API Gateway and AWS Lambda for request authorization. Additionally, the application uses a service mesh pattern to facilitate communication between microservices deployed on Amazon ECS. Considering cost-effectiveness, low latency, and secure access, which approach should you take? (Select ONE.)", "choices": ["Use API Gateway with a Lambda authorizer for authentication, integrate with AWS App Mesh for microservice communication, and enable VPC Link for direct VPC access.", "Use API Gateway with a Lambda authorizer for authentication, integrate with AWS App Mesh for microservice communication, and use AWS Direct Connect for low-latency network connections.", "Use API Gateway with a Lambda authorizer for authentication, leverage Amazon VPC Peering for microservice communication, and use AWS CloudFront for caching API responses.", "Use API Gateway with a Lambda authorizer for authentication, integrate with AWS App Mesh for microservice communication, and use AWS Global Accelerator for improved performance.", "Use API Gateway with a Lambda authorizer for authentication, integrate directly with Amazon ECS services without a service mesh, and use AWS Shield Advanced for DDoS protection."], "target": "A", "metadata": {"difficulty": "sa_pro", "domain": "integration", "aws_services": ["Amazon API Gateway", "AWS Lambda", "AWS App Mesh", "Amazon ECS", "VPC Link"]}}
{"input": "A large enterprise needs to set up an event-driven architecture that can route events across multiple AWS accounts in a cost-effective manner. The solution must ensure minimal latency and comply with regulatory requirements that mandate encrypted event data in transit. How can they achieve this using Amazon EventBridge? (Select TWO.)", "choices": ["Use Amazon EventBridge to create an event bus in each account and set up resource policies to allow cross-account event sharing.", "Configure AWS Lambda functions in each account to pull events from a shared S3 bucket and process them.", "Utilize Amazon EventBridge's cross-account event sharing feature by setting up event rules and resource-based policies.", "Deploy AWS Step Functions across accounts to orchestrate and route the events securely.", "Implement AWS PrivateLink to establish secure, low-latency connections between event buses in different accounts."], "target": ["A", "C"], "metadata": {"difficulty": "sa_pro", "domain": "integration", "aws_services": ["Amazon EventBridge", "AWS Lambda", "Amazon S3", "AWS Step Functions", "AWS PrivateLink"]}}
{"input": "An enterprise is deploying a machine learning pipeline on AWS using Amazon SageMaker for model training and inference. The pipeline needs to include model monitoring and automated retraining triggered by data drift. The company has strict requirements for minimizing operational costs and ensuring compliance with data privacy regulations. Which architecture would best meet these requirements? (Select ONE.)", "choices": ["Use SageMaker Model Monitor to track data drift and integrate with AWS Lambda to trigger a new training job. Store training data in Amazon S3 and use SageMaker Processing for preprocessing.", "Use SageMaker Model Monitor to track data drift and integrate with AWS Step Functions to orchestrate retraining. Store training data in Amazon RDS and use SageMaker Data Wrangler for preprocessing.", "Use SageMaker Model Monitor to track data drift and integrate with Amazon EventBridge to trigger a new training job. Store training data in Amazon S3 for cost-effective storage and use SageMaker Processing for preprocessing.", "Use SageMaker Model Monitor to track data drift and integrate with Amazon ECS to run containers for retraining. Store training data in Amazon DynamoDB and use SageMaker Data Wrangler for preprocessing."], "target": "C", "metadata": {"difficulty": "sa_pro", "domain": "ml_ai", "aws_services": ["Amazon SageMaker", "Amazon S3", "AWS Lambda", "AWS Step Functions", "Amazon RDS", "Amazon EventBridge", "Amazon ECS", "Amazon DynamoDB"]}}
{"input": "An enterprise is looking to integrate Amazon Bedrock with its existing customer service application to leverage generative AI for better customer interactions. The application is hosted on Amazon ECS with data stored in Amazon S3. The enterprise requires low latency responses and high availability. Additionally, compliance with GDPR is crucial. Which architectural components should be included to meet these requirements? (Select THREE.)", "choices": ["Use AWS Lambda to trigger Amazon Bedrock model inference upon incoming requests.", "Deploy Amazon Bedrock in multiple regions and use Amazon Route 53 for latency-based routing.", "Configure Amazon ECS Service Auto Scaling to ensure high availability.", "Implement AWS CloudTrail to monitor requests for compliance purposes.", "Use AWS Direct Connect for low latency connectivity between the application and Amazon Bedrock.", "Store all inference results in an Amazon DynamoDB table for audit and compliance tracking."], "target": ["B", "C", "D"], "metadata": {"difficulty": "sa_pro", "domain": "ml_ai", "aws_services": ["Amazon Bedrock", "Amazon ECS", "Amazon S3", "Amazon Route 53", "AWS CloudTrail", "Amazon DynamoDB"]}}
{"input": "An enterprise is building a data lake architecture using AWS Lake Formation and Amazon Redshift to analyze large datasets. The company requires low latency for analytical queries, needs to ensure compliance with data governance policies, and aims to minimize costs. Which architectural approach should be adopted to meet these requirements?", "choices": ["Store raw data in Amazon S3, use AWS Glue for ETL processes, and query transformed data with Amazon Athena for low-latency analytics.", "Use AWS Lake Formation to manage Amazon S3 permissions, leverage Amazon Redshift Spectrum to directly query data in S3, enabling low-latency analytics and cost-effective storage.", "Store data in Amazon S3 and replicate it to Amazon Redshift using AWS Data Pipeline for low-latency queries, ensuring compliance through AWS IAM policies.", "Use AWS Lake Formation with Amazon EMR for ETL processes, and load data into Amazon Redshift for analytics, ensuring compliance with AWS KMS encryption."], "target": "B", "metadata": {"difficulty": "sa_pro", "domain": "data_analytics", "aws_services": ["AWS Lake Formation", "Amazon Redshift", "Amazon Redshift Spectrum", "Amazon S3", "AWS Glue", "Amazon Athena", "AWS Data Pipeline", "Amazon EMR", "AWS KMS"]}}
{"input": "A financial services company needs to process real-time streaming data from its trading platform using AWS. They require low latency and high throughput for their analytics. Data must be stored securely, and the solution should be cost-effective. Which combination of AWS services and features should they use to achieve this? (Select TWO.)", "choices": ["Use Amazon Kinesis Data Streams for ingesting streaming data and Amazon Kinesis Data Analytics for real-time processing.", "Use AWS Lambda and Amazon DynamoDB Streams for processing data in real-time.", "Use Amazon Kinesis Data Firehose to ingest data and store it in Amazon S3 for batch processing.", "Use Amazon Kinesis Data Streams with Apache Flink applications running on Amazon Kinesis Data Analytics for real-time processing.", "Use Amazon EMR with Apache Spark for real-time stream processing and store results in Amazon Redshift.", "Deploy Amazon EC2 instances running Apache Kafka for ingesting data and processing using Apache Flink."], "target": ["A", "D"], "metadata": {"difficulty": "sa_pro", "domain": "data_analytics", "aws_services": ["Amazon Kinesis Data Streams", "Amazon Kinesis Data Analytics", "AWS Lambda", "Amazon DynamoDB Streams", "Amazon Kinesis Data Firehose", "Amazon S3", "Amazon EMR", "Amazon Redshift", "Amazon EC2"]}}
{"input": "An enterprise needs to securely share Amazon S3 bucket data with another AWS account. The solution must ensure that the sharing mechanism is scalable and maintains compliance with the organization's security policies, which include the principle of least privilege and auditability. The organization is concerned about the potential complexity and cost of managing cross-account permissions. Which solution best addresses these requirements?", "choices": ["Use AWS Organizations to automatically share the S3 bucket with the other account.", "Create an IAM role in the source account with permissions to access the S3 bucket, and allow the other account to assume this role using a trust policy.", "Use a bucket policy that grants the other account access to the specific S3 bucket resources.", "Enable S3 cross-region replication to copy the data to a bucket in the other account."], "target": "B", "metadata": {"difficulty": "sa_pro", "domain": "security", "aws_services": ["Amazon S3", "AWS IAM"]}}
{"input": "An enterprise is deploying a microservices application across multiple VPCs within the same AWS region and requires a Zero Trust security model. The application needs secure and private communication between the microservices without exposing them to the internet. Additionally, the architecture must comply with data protection regulations and minimize latency. Which of the following configurations will best meet these requirements? (Select TWO.)", "choices": ["Use AWS PrivateLink to create VPC endpoints for each microservice, allowing private communication between VPCs.", "Deploy a transit gateway to route traffic between VPCs and use security groups to restrict access.", "Configure VPC peering connections between all VPCs and use network ACLs to enforce security policies.", "Implement AWS Direct Connect with VPN to establish secure connections between VPCs.", "Use AWS PrivateLink to access AWS services such as Amazon S3 securely without traversing the internet."], "target": ["A", "E"], "metadata": {"difficulty": "sa_pro", "domain": "security", "aws_services": ["AWS PrivateLink", "VPC", "AWS Direct Connect", "Amazon S3"]}}
{"input": "An e-commerce company is designing a serverless application to process customer orders. The application needs to perform the following actions: validate the order, check inventory, process payment, and update the order status. All steps must be reliably executed in sequence, and failures in any step should trigger an alert without affecting the other steps. The solution must ensure low latency and scalability while minimizing operational overhead. Which combination of AWS services and configurations should be used to meet these requirements? (Select TWO.)", "choices": ["Use AWS Step Functions to orchestrate the workflow and Amazon SNS to send alerts for failures.", "Use AWS Lambda functions to handle each step, with Amazon SQS to manage message queuing between steps.", "Use Amazon EventBridge to trigger AWS Lambda functions for each step and AWS CloudTrail to monitor execution.", "Use AWS Step Functions with built-in error handling and Amazon EventBridge to trigger alert notifications.", "Use AWS Step Functions for workflow orchestration and AWS Step Functions' built-in retry mechanisms for error handling."], "target": ["A", "D"], "metadata": {"difficulty": "sa_pro", "domain": "serverless", "aws_services": ["AWS Step Functions", "Amazon SNS", "AWS Lambda", "Amazon SQS", "Amazon EventBridge", "AWS CloudTrail"]}}
{"input": "Your organization has multiple AWS accounts and wants to optimize costs across these accounts. They are considering using AWS services that can provide consolidated billing and allow sharing of reserved instances across accounts. Additionally, they want to ensure that the solution supports compliance requirements and enables detailed cost tracking and reporting. Which solution should they implement to meet these requirements?", "choices": ["Use AWS Cost Explorer in each account separately and purchase Reserved Instances in each account.", "Set up AWS Organizations with consolidated billing and use AWS Cost Explorer for detailed cost tracking. Share Reserved Instances across accounts using AWS Resource Access Manager.", "Use Amazon CloudWatch to monitor costs and set up alarms for each account. Purchase Reserved Instances separately in each account.", "Use AWS Budgets to set cost limits for each account and manage Reserved Instances separately without sharing."], "target": "B", "metadata": {"difficulty": "sa_pro", "domain": "architecture", "aws_services": ["AWS Cost Explorer", "AWS Organizations", "AWS Resource Access Manager", "Amazon CloudWatch", "AWS Budgets"]}}
{"input": "A multinational corporation is designing a hybrid cloud architecture to securely connect their on-premises data centers to AWS. They require low latency, high throughput, and reliable connectivity between AWS and their data centers. The solution must also allow centralized management of connectivity across multiple AWS accounts. Which combination of AWS services should they use to meet these requirements? (Select TWO.)", "choices": ["AWS Direct Connect", "AWS VPN", "AWS Transit Gateway", "Amazon Route 53", "AWS CloudFormation"], "target": ["A", "C"], "metadata": {"difficulty": "sa_pro", "domain": "architecture", "aws_services": ["AWS Direct Connect", "AWS Transit Gateway"]}}
